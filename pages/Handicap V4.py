##################### BLOCO 1 ‚Äì IMPORTS & CONFIG #####################
import streamlit as st
import pandas as pd
import numpy as np
import os
import joblib
import re
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, log_loss, brier_score_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.calibration import CalibratedClassifierCV
from datetime import datetime, timedelta

st.set_page_config(page_title="Bet Indicator ‚Äì Asian Handicap", layout="wide")
st.title("üìä Bet Indicator ‚Äì Asian Handicap (Home vs Away)")

# ---------------- Configura√ß√µes ----------------
PAGE_PREFIX = "AsianHandicap"
GAMES_FOLDER = "GamesDay"
LIVESCORE_FOLDER = "LiveScore"
EXCLUDED_LEAGUE_KEYWORDS = ["cup", "copas", "uefa", "afc", "sudamericana", "copa","trophy"]

BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
MODELS_FOLDER = os.path.join(BASE_DIR, "Models")
os.makedirs(MODELS_FOLDER, exist_ok=True)


##################### BLOCO 2 ‚Äì HELPERS #####################
def preprocess_df(df):
    df = df.copy()
    if "Goals_H_FT_x" in df.columns:
        df = df.rename(columns={"Goals_H_FT_x": "Goals_H_FT", "Goals_A_FT_x": "Goals_A_FT"})
    elif "Goals_H_FT_y" in df.columns:
        df = df.rename(columns={"Goals_H_FT_y": "Goals_H_FT", "Goals_A_FT_y": "Goals_A_FT"})
    return df

def load_all_games(folder):
    files = [f for f in os.listdir(folder) if f.endswith(".csv")]
    if not files:
        return pd.DataFrame()
    dfs = [preprocess_df(pd.read_csv(os.path.join(folder, f))) for f in files]
    return pd.concat(dfs, ignore_index=True)

def load_selected_csvs(folder):
    files = sorted([f for f in os.listdir(folder) if f.endswith(".csv")])
    if not files:
        return pd.DataFrame()
    dfs = [preprocess_df(pd.read_csv(os.path.join(folder, f))) for f in files]
    return pd.concat(dfs, ignore_index=True)

def filter_leagues(df):
    if df.empty or "League" not in df.columns:
        return df
    pattern = "|".join(EXCLUDED_LEAGUE_KEYWORDS)
    return df[~df["League"].str.lower().str.contains(pattern, na=False)].copy()

def save_model(model, feature_cols, filename):
    with open(os.path.join(MODELS_FOLDER, filename), "wb") as f:
        joblib.dump((model, feature_cols), f)

def load_model(filename):
    path = os.path.join(MODELS_FOLDER, filename)
    if os.path.exists(path):
        with open(path, "rb") as f:
            return joblib.load(f)
    return None


##################### BLOCO 3 ‚Äì LOAD DATA + HANDICAP TARGET #####################
st.info("üìÇ Loading data...")

# ========== SELE√á√ÉO DE DATA ==========
files = [f for f in os.listdir(GAMES_FOLDER) if f.endswith(".csv")]
files = sorted(files)

if not files:
    st.warning("No CSV files found in GamesDay folder.")
    st.stop()

# √öltimos dois arquivos (Hoje e Ontem)
options = files[-2:] if len(files) >= 2 else files
selected_file = st.selectbox("Select Matchday File:", options, index=len(options)-1)

# Extrair a data do arquivo selecionado (YYYY-MM-DD)
date_match = re.search(r"\d{4}-\d{2}-\d{2}", selected_file)
if date_match:
    selected_date_str = date_match.group(0)
else:
    selected_date_str = datetime.now().strftime("%Y-%m-%d")

# Carregar os jogos do dia selecionado
games_today = pd.read_csv(os.path.join(GAMES_FOLDER, selected_file))
games_today = filter_leagues(games_today)

# ========== MERGE COM LIVESCORE ==========
livescore_file = os.path.join(LIVESCORE_FOLDER, f"Resultados_RAW_{selected_date_str}.csv")

# Ensure goal columns exist
if 'Goals_H_Today' not in games_today.columns:
    games_today['Goals_H_Today'] = np.nan
if 'Goals_A_Today' not in games_today.columns:
    games_today['Goals_A_Today'] = np.nan

# Merge with the correct LiveScore file
if os.path.exists(livescore_file):
    st.info(f"LiveScore file found: {livescore_file}")
    results_df = pd.read_csv(livescore_file)

    # FILTER OUT CANCELED AND POSTPONED GAMES
    results_df = results_df[~results_df['status'].isin(['Cancel', 'Postp.'])]
    
    required_cols = [
        'game_id', 'status', 'home_goal', 'away_goal',
        'home_ht_goal', 'away_ht_goal',
        'home_corners', 'away_corners',
        'home_yellow', 'away_yellow',
        'home_red', 'away_red'
    ]
    missing_cols = [col for col in required_cols if col not in results_df.columns]
    
    if missing_cols:
        st.error(f"The file {livescore_file} is missing these columns: {missing_cols}")
    else:
        games_today = games_today.merge(
            results_df,
            left_on='Id',
            right_on='game_id',
            how='left',
            suffixes=('', '_RAW')
        )

        # Update goals only for finished games
        games_today['Goals_H_Today'] = games_today['home_goal']
        games_today['Goals_A_Today'] = games_today['away_goal']
        games_today.loc[games_today['status'] != 'FT', ['Goals_H_Today', 'Goals_A_Today']] = np.nan
        
        # ADD RED CARD COLUMNS
        games_today['Home_Red'] = games_today['home_red']
        games_today['Away_Red'] = games_today['away_red']
else:
    st.warning(f"No LiveScore results file found for selected date: {selected_date_str}")

# ========== CARREGAR HIST√ìRICO ==========
history = filter_leagues(load_all_games(GAMES_FOLDER))
history = history.dropna(subset=["Goals_H_FT", "Goals_A_FT", "Asian_Line"]).copy()

if set(["Date", "Home", "Away"]).issubset(history.columns):
    history = history.drop_duplicates(subset=["Home", "Away","Goals_H_FT", "Goals_A_FT"], keep="first")
else:
    history = history.drop_duplicates(keep="first")

if history.empty:
    st.stop()

# Filtrar apenas jogos sem placar final (para previs√£o)
if "Goals_H_FT" in games_today.columns:
    games_today = games_today[games_today["Goals_H_FT"].isna()].copy()

if games_today.empty:
    st.warning("‚ö†Ô∏è No matches found for today (or yesterday, if selected).")
    st.stop()

def convert_asian_line(line_str):
    try:
        if pd.isna(line_str) or line_str == "":
            return None
        line_str = str(line_str).strip()
        if "/" not in line_str:
            return float(line_str)
        parts = [float(x) for x in line_str.split("/")]
        return sum(parts) / len(parts)
    except:
        return None

history["Asian_Line_Display"] = history["Asian_Line"].apply(convert_asian_line)
games_today["Asian_Line_Display"] = games_today["Asian_Line"].apply(convert_asian_line)

def calc_handicap_result(margin, asian_line_str, invert=False):
    if pd.isna(asian_line_str):
        return np.nan
    if invert:
        margin = -margin
    try:
        parts = [float(x) for x in str(asian_line_str).split('/')]
    except:
        return np.nan
    results = []
    for line in parts:
        if margin > line:
            results.append(1.0)
        elif margin == line:
            results.append(0.5)
        else:
            results.append(0.0)
    return np.mean(results)

history["Margin"] = history["Goals_H_FT"] - history["Goals_A_FT"]
history["Handicap_Home_Result"] = history.apply(lambda r: calc_handicap_result(r["Margin"], r["Asian_Line"], invert=False), axis=1)
history["Handicap_Away_Result"] = history.apply(lambda r: calc_handicap_result(r["Margin"], r["Asian_Line"], invert=True), axis=1)

history["Target_AH_Home"] = history["Handicap_Home_Result"].apply(lambda x: 1 if x >= 0.5 else 0)
history["Target_AH_Away"] = history["Handicap_Away_Result"].apply(lambda x: 1 if x >= 0.5 else 0)


##################### BLOCO 4 ‚Äì FEATURE ENGINEERING OTIMIZADO #####################

# PRIMEIRO definir a lista de features
away_premium_features = [
    'Underdog_Indicator',      # Correla√ß√£o 0.261 ‚úÖ
    'Handicap_Balance',        # Correla√ß√£o -0.261 ‚úÖ
    'Aggression_Away',         # Correla√ß√£o 0.209 ‚úÖ
    'Aggression_Home',         # Correla√ß√£o -0.190 ‚úÖ
    'Odd_A',                   # Contexto de odds
    'Asian_Line_Display',      # Linha do handicap
    'Odds_Ratio',              # Rela√ß√£o de for√ßas
    'Line_Abs'                 # Magnitude do handicap
]

def create_optimized_features(df):
    """
    Feature engineering focado nas vari√°veis com correla√ß√£o comprovada
    """
    df = df.copy()
    
    # ‚úÖ FEATURES COM CORRELA√á√ÉO FORTE (Foco Away)
    # Verificar se as colunas existem antes de criar features
    if all(col in df.columns for col in ['Aggression_Home', 'Aggression_Away']):
        df['Underdog_Indicator'] = df['Aggression_Away'] - df['Aggression_Home']
        df['Handicap_Balance'] = df['Aggression_Home'] - df['Aggression_Away']
        st.write("‚úÖ Features de Aggression criadas")
    else:
        st.warning("‚ö†Ô∏è Colunas de Aggression n√£o encontradas")
    
    # ‚úÖ FEATURES COMPLEMENTARES
    if all(col in df.columns for col in ['Odd_H', 'Odd_A']):
        df['Odds_Ratio'] = df['Odd_A'] / df['Odd_H']
        st.write("‚úÖ Odds_Ratio criada")
    
    # ‚úÖ FEATURE DE LINHA (importante para handicap)
    if 'Asian_Line_Display' in df.columns:
        df['Line_Abs'] = abs(df['Asian_Line_Display'])
        st.write("‚úÖ Line_Abs criada")
    else:
        st.warning("‚ö†Ô∏è Asian_Line_Display n√£o encontrado")
    
    return df

# Aplicar feature engineering otimizado
st.info("üîÑ Aplicando feature engineering otimizado...")

history = create_optimized_features(history)
games_today = create_optimized_features(games_today)

# AGORA filtrar apenas features que existem
away_premium_features = [f for f in away_premium_features if f in history.columns]

st.success(f"‚úÖ Features premium para Away Handicap: {away_premium_features}")

if not away_premium_features:
    st.error("‚ùå Nenhuma feature premium dispon√≠vel!")
    st.stop()

##################### BLOCO 4.1 ‚Äì PREPARAR DADOS PARA MODELO AWAY PREMIUM #####################

# Garantir que temos o target para Away
if "Target_AH_Away" not in history.columns:
    st.error("Target_AH_Away n√£o encontrado no hist√≥rico!")
    st.stop()

# VERIFICA√á√ÉO ROBUSTA DAS FEATURES
st.info("üîç Verificando disponibilidade das features...")

# Mostrar todas as colunas dispon√≠veis para debug
st.write("üìã **Colunas dispon√≠veis no hist√≥rico:**", list(history.columns))
st.write("üìã **Colunas dispon√≠veis hoje:**", list(games_today.columns))

# Features que realmente existem no hist√≥rico
available_away_features = [f for f in away_premium_features if f in history.columns]
st.write(f"‚úÖ Features dispon√≠veis no hist√≥rico: {available_away_features}")

# Features que existem nos dados de hoje
available_today_features = [f for f in away_premium_features if f in games_today.columns]
st.write(f"‚úÖ Features dispon√≠veis hoje: {available_today_features}")

# Usar apenas as features que existem em AMBOS
final_away_features = [f for f in available_away_features if f in available_today_features]
st.success(f"üéØ Features finais para o modelo: {final_away_features}")

if not final_away_features:
    st.error("‚ùå Nenhuma feature dispon√≠vel em ambos hist√≥rico e dados de hoje!")
    
    # Mostrar quais features est√£o faltando
    missing_in_today = [f for f in available_away_features if f not in available_today_features]
    if missing_in_today:
        st.write(f"‚ùå Features faltando em games_today: {missing_in_today}")
    
    # Tentar uma abordagem alternativa com features b√°sicas
    st.info("üîÑ Tentando com features b√°sicas...")
    basic_features = ['Odd_A', 'Asian_Line_Display']
    final_away_features = [f for f in basic_features if f in history.columns and f in games_today.columns]
    
    if final_away_features:
        st.success(f"üéØ Usando features b√°sicas: {final_away_features}")
    else:
        st.stop()

# VERIFICA√á√ÉO FINAL - garantir que todas as features existem
missing_in_history = [f for f in final_away_features if f not in history.columns]
missing_in_today = [f for f in final_away_features if f not in games_today.columns]

if missing_in_history:
    st.error(f"‚ùå Features faltando no hist√≥rico: {missing_in_history}")
    final_away_features = [f for f in final_away_features if f not in missing_in_history]

if missing_in_today:
    st.error(f"‚ùå Features faltando em games_today: {missing_in_today}")
    final_away_features = [f for f in final_away_features if f not in missing_in_today]

if not final_away_features:
    st.error("‚ùå Nenhuma feature dispon√≠vel ap√≥s verifica√ß√£o!")
    st.stop()

st.success(f"‚úÖ Features confirmadas: {final_away_features}")

# Preparar matriz de features para Away
try:
    X_away = history[final_away_features].copy()
    y_away = history["Target_AH_Away"].copy()
    st.success("‚úÖ Dados hist√≥ricos preparados com sucesso")
except KeyError as e:
    st.error(f"‚ùå Erro ao preparar dados hist√≥ricos: {e}")
    st.stop()

# One-hot encoding para ligas
try:
    history_leagues = pd.get_dummies(history["League"], prefix="League")
    games_today_leagues = pd.get_dummies(games_today["League"], prefix="League")
    
    # Garantir que as colunas de liga s√£o as mesmas
    common_league_cols = list(set(history_leagues.columns) & set(games_today_leagues.columns))
    if not common_league_cols:
        st.warning("‚ö†Ô∏è Nenhuma liga comum entre hist√≥rico e dados de hoje")
        history_leagues = pd.DataFrame()
        games_today_leagues = pd.DataFrame()
    else:
        games_today_leagues = games_today_leagues.reindex(columns=common_league_cols, fill_value=0)
        history_leagues = history_leagues[common_league_cols]
        st.success(f"‚úÖ {len(common_league_cols)} ligas comuns encontradas")
        
except Exception as e:
    st.warning(f"‚ö†Ô∏è Erro no encoding de ligas: {e}")
    history_leagues = pd.DataFrame()
    games_today_leagues = pd.DataFrame()

# Adicionar ligas √†s features (se existirem)
if not history_leagues.empty:
    X_away = pd.concat([X_away, history_leagues], axis=1)
    league_cols = history_leagues.columns.tolist()
    final_away_features.extend(league_cols)
    st.success(f"‚úÖ Adicionadas {len(league_cols)} colunas de liga")

# Preparar dados de hoje com VERIFICA√á√ÉO
try:
    X_today_away = games_today[final_away_features].copy()
    
    # Adicionar ligas se existirem
    if not games_today_leagues.empty:
        X_today_away = pd.concat([X_today_away, games_today_leagues], axis=1)
    
    # Garantir que as colunas s√£o as mesmas
    X_today_away = X_today_away.reindex(columns=X_away.columns, fill_value=0)
    st.success("‚úÖ Dados de hoje preparados com sucesso")
    
except KeyError as e:
    st.error(f"‚ùå Erro ao preparar dados de hoje: {e}")
    
    # Tentar alternativa: usar apenas as colunas que existem
    existing_cols = [col for col in X_away.columns if col in games_today.columns]
    if existing_cols:
        st.info(f"üîÑ Usando apenas {len(existing_cols)} colunas dispon√≠veis")
        X_today_away = games_today[existing_cols].copy()
        X_today_away = X_today_away.reindex(columns=X_away.columns, fill_value=0)
    else:
        st.error("‚ùå Nenhuma coluna comum dispon√≠vel")
        st.stop()

# Normaliza√ß√£o das features num√©ricas
numeric_away_features = [f for f in final_away_features if f in X_away.columns and 
                        X_away[f].dtype in ['float64', 'int64'] and 
                        not f.startswith('League_')]

st.info(f"üî¢ Features num√©ricas para normaliza√ß√£o: {numeric_away_features}")

# Mostrar estat√≠sticas das features
st.write("üìä Estat√≠sticas das features no hist√≥rico:")
st.dataframe(X_away.describe(), use_container_width=True)

# Mostrar shape dos dados
st.write(f"üìê Shape X_away: {X_away.shape}")
st.write(f"üìê Shape X_today_away: {X_today_away.shape}")
st.write(f"üìê Shape y_away: {y_away.shape}")

##################### BLOCO 5 ‚Äì MODELO AWAY PREMIUM #####################

def train_away_premium_model(X, y, retrain=False):
    """
    Modelo especializado para Away Handicap
    """
    filename = f"AsianHandicap_Away_Premium_XGB_v2.pkl"
    
    if not retrain:
        loaded = load_model(filename)
        if loaded:
            model, feature_cols = loaded
            st.success("‚úÖ Modelo Away Premium carregado do cache")
            return model, feature_cols
    
    # DEBUG: Verificar dados antes do split
    st.write("üîç **Debug - Antes do split:**")
    st.write(f"Shape X: {X.shape}, Shape y: {y.shape}")
    st.write(f"Tipos de X: {X.dtypes}")
    st.write(f"Valores √∫nicos em y: {y.unique()}, Counts: {y.value_counts()}")
    
    # Verificar se h√° dados suficientes
    if len(X) < 100:
        st.error(f"‚ùå Dados insuficientes para treinamento: apenas {len(X)} amostras")
        return None, None
    
    # Verificar se y tem pelo menos 2 classes
    if len(y.unique()) < 2:
        st.error(f"‚ùå Target precisa ter pelo menos 2 classes. Encontrado: {y.unique()}")
        return None, None
    
    # Split temporal (mais realista)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]
    y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]
    
    st.write("üîç **Debug - Ap√≥s split:**")
    st.write(f"X_train: {X_train.shape}, X_test: {X_test.shape}")
    st.write(f"y_train counts: {y_train.value_counts()}")
    st.write(f"y_test counts: {y_test.value_counts()}")
    
    # Verificar NaN e infinitos
    st.write("üîç **Verifica√ß√£o de dados:**")
    st.write(f"NaN em X_train: {X_train.isna().sum().sum()}")
    st.write(f"NaN em X_test: {X_test.isna().sum().sum()}")
    st.write(f"NaN em y_train: {y_train.isna().sum()}")
    st.write(f"NaN em y_test: {y_test.isna().sum()}")
    
    # Limpar dados
    X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)
    X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)
    
    # Normaliza√ß√£o
    if normalize_features and numeric_away_features:
        # Verificar se as colunas num√©ricas existem
        available_numeric = [f for f in numeric_away_features if f in X_train.columns]
        if available_numeric:
            scaler = StandardScaler()
            X_train[available_numeric] = scaler.fit_transform(X_train[available_numeric])
            X_test[available_numeric] = scaler.transform(X_test[available_numeric])
            
            # Salvar scaler para uso futuro
            joblib.dump(scaler, os.path.join(MODELS_FOLDER, "away_premium_scaler.pkl"))
            st.success(f"‚úÖ Normalizadas {len(available_numeric)} features")
        else:
            st.warning("‚ö†Ô∏è Nenhuma feature num√©rica dispon√≠vel para normaliza√ß√£o")
    
    # Modelo XGBoost otimizado para Away - CONFIGURA√á√ÉO SIMPLIFICADA
    try:
        model = XGBClassifier(
            n_estimators=100,  # Reduzido para teste
            max_depth=4,       # Reduzido para teste
            learning_rate=0.1,
            eval_metric='logloss',
            random_state=42
        )
        
        st.write("üîç **Iniciando treinamento...**")
        
        # Treinar sem early stopping primeiro
        model.fit(X_train, y_train)
        st.success("‚úÖ Treinamento conclu√≠do com sucesso!")
        
    except Exception as e:
        st.error(f"‚ùå Erro no treinamento: {e}")
        
        # Tentar com Random Forest como fallback
        st.info("üîÑ Tentando com Random Forest...")
        from sklearn.ensemble import RandomForestClassifier
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=5,
            random_state=42
        )
        model.fit(X_train, y_train)
        st.success("‚úÖ Random Forest treinado com sucesso!")
    
    # Avalia√ß√£o
    try:
        preds = model.predict(X_test)
        probs = model.predict_proba(X_test)
        
        st.write("üìä **Performance do Modelo Away Premium:**")
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("Acur√°cia", f"{accuracy_score(y_test, preds):.1%}")
        with col2:
            st.metric("Log Loss", f"{log_loss(y_test, probs):.3f}")
        with col3:
            st.metric("Brier Score", f"{brier_score_loss(y_test, probs[:,1]):.3f}")
        
    except Exception as e:
        st.error(f"‚ùå Erro na avalia√ß√£o: {e}")
    
    # Feature importance (se dispon√≠vel)
    try:
        if hasattr(model, 'feature_importances_'):
            feature_importance = pd.DataFrame({
                'feature': X.columns,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            st.write("üéØ **Top 10 Features Mais Importantes:**")
            st.dataframe(feature_importance.head(10), use_container_width=True)
    except Exception as e:
        st.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel calcular feature importance: {e}")
    
    # Salvar modelo
    try:
        save_model(model, X.columns.tolist(), filename)
        st.success("‚úÖ Modelo Away Premium salvo")
    except Exception as e:
        st.error(f"‚ùå Erro ao salvar modelo: {e}")
    
    return model, X.columns.tolist()

##################### BLOCO 6 ‚Äì SISTEMA DE CONFIAN√áA INTEGRADO #####################

def calculate_confidence_system(row, prob_away):
    """
    Sistema de confian√ßa integrado para Away + oportunidades Home
    """
    # 1. CONFIAN√áA AWAY HANDICAP
    away_confidence = "BAIXA"
    away_reason = []
    
    # Usar get() com valores padr√£o para evitar KeyError
    underdog_indicator = row.get('Underdog_Indicator', 0)
    aggression_away = row.get('Aggression_Away', 0)
    aggression_home = row.get('Aggression_Home', 0)
    
    if (underdog_indicator > 0.3 and 
        prob_away > 0.60 and
        aggression_away > 0.2):
        away_confidence = "ALTA"
        away_reason.append("Underdog claro com aggression away alta")
    
    elif (underdog_indicator > 0.15 and 
          prob_away > 0.55 and
          aggression_away > 0.1):
        away_confidence = "M√âDIA"
        away_reason.append("Underdog moderado com probabilidade boa")
    
    else:
        away_confidence = "BAIXA"
        away_reason.append("Indicadores fracos para Away")
    
    # 2. OPORTUNIDADE HOME HANDICAP (baseado na fraqueza do Away)
    home_opportunity = "FRACA"
    home_reason = []
    
    if (prob_away < 0.35 and 
        underdog_indicator < -0.2 and
        aggression_home > aggression_away):
        home_opportunity = "FORTE"
        home_reason.append("Away muito fraco e Home favorito claro")
    
    elif (prob_away < 0.45 and 
          underdog_indicator < 0):
        home_opportunity = "MODERADA"
        home_reason.append("Away fraco e Home n√£o √© underdog")
    
    else:
        home_opportunity = "FRACA"
        home_reason.append("Away n√£o est√° suficientemente fraco")
    
    return {
        'away_confidence': away_confidence,
        'away_reason': " | ".join(away_reason),
        'home_opportunity': home_opportunity,
        'home_reason': " | ".join(home_reason)
    }

##################### BLOCO 7 ‚Äì SIDEBAR CONFIG #####################
st.sidebar.header("‚öôÔ∏è Settings")
ml_model_choice = st.sidebar.selectbox("Choose ML Model", ["Random Forest", "XGBoost"])
ml_version_choice = st.sidebar.selectbox("Choose Model Version", ["v1", "v2"])
retrain = st.sidebar.checkbox("Retrain models", value=False)
normalize_features = st.sidebar.checkbox("Normalize features (odds + strength + aggression)", value=True)

# NOVA CONFIGURA√á√ÉO: Estrat√©gia
strategy_choice = st.sidebar.selectbox(
    "üéØ Estrat√©gia", 
    ["Away Premium + Home Opportunities", "Original Both Sides"]
)

##################### BLOCO 8 ‚Äì APLICA√á√ÉO DO MODELO AWAY PREMIUM #####################

if strategy_choice == "Away Premium + Home Opportunities":
    # VERIFICA√á√ÉO FINAL ANTES DO TREINAMENTO
    st.info("üîç Verificando dados antes do treinamento...")
    
    # Verificar se temos dados suficientes
    if len(X_away) < 50:
        st.error(f"‚ùå Dados hist√≥ricos insuficientes: apenas {len(X_away)} amostras")
        st.info("‚ö†Ô∏è Usando estrat√©gia original por falta de dados")
        strategy_choice = "Original Both Sides"
    elif len(y_away.unique()) < 2:
        st.error(f"‚ùå Target com apenas uma classe: {y_away.unique()}")
        st.info("‚ö†Ô∏è Usando estrat√©gia original por problema no target")
        strategy_choice = "Original Both Sides"
    else:
        st.info("ü§ñ Treinando modelo Away Premium...")
        away_model, away_feature_cols = train_away_premium_model(X_away, y_away, retrain)
        
        if away_model is None:
            st.error("‚ùå Falha no treinamento do modelo Away Premium")
            st.info("‚ö†Ô∏è Voltando para estrat√©gia original")
            strategy_choice = "Original Both Sides"

    ##################### BLOCO 9 ‚Äì VISUALIZA√á√ÉO DOS RESULTADOS PREMIUM #####################

    st.markdown(f"## üéØ PREVIS√ïES AWAY HANDICAP + OPORTUNIDADES HOME - {selected_date_str}")

    # Fun√ß√£o para colorir basedo na confian√ßa
    def color_confidence(val):
        if val == 'ALTA' or val == 'FORTE':
            return 'background-color: #4CAF50; color: white; font-weight: bold;'
        elif val == 'M√âDIA' or val == 'MODERADA':
            return 'background-color: #FF9800; color: white; font-weight: bold;'
        else:
            return 'background-color: #F44336; color: white;'

    # DataFrame final otimizado
    display_df = games_today[['Home', 'Away', 'League', 'Asian_Line_Display']].copy()
    display_df['Match'] = display_df['Home'] + ' vs ' + display_df['Away']

    # Adicionar colunas calculadas
    display_df['Prob Away HC'] = games_today['p_ah_away_yes']
    display_df['Confian√ßa Away'] = games_today['away_confidence']
    display_df['Stake Away'] = games_today['stake_away']
    display_df['Prob Home HC'] = games_today['prob_home']
    display_df['Oportunidade Home'] = games_today['home_opportunity']
    display_df['Stake Home'] = games_today['stake_home']

    # Ordenar por confian√ßa Away (mais altos primeiro)
    display_df = display_df.sort_values(['Stake Away', 'Stake Home'], ascending=[False, False])

    # Exibir resultados
    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown("### üìä Previs√µes Detalhadas")
        
        # Formata√ß√£o da tabela
        styled_df = display_df[[
            'Match', 'League', 'Asian_Line_Display',
            'Prob Away HC', 'Confian√ßa Away', 'Stake Away',
            'Prob Home HC', 'Oportunidade Home', 'Stake Home'
        ]].style.format({
            'Prob Away HC': '{:.1%}',
            'Prob Home HC': '{:.1%}',
            'Asian_Line_Display': '{:.2f}',
            'Stake Away': 'R$ {:.0f}',
            'Stake Home': 'R$ {:.0f}'
        }).applymap(color_confidence, subset=['Confian√ßa Away', 'Oportunidade Home'])
        
        st.dataframe(styled_df, use_container_width=True, height=600)

    with col2:
        st.markdown("### üéØ Resumo de Oportunidades")
        
        # Estat√≠sticas r√°pidas
        high_away = len(display_df[display_df['Confian√ßa Away'] == 'ALTA'])
        strong_home = len(display_df[display_df['Oportunidade Home'] == 'FORTE'])
        
        st.metric("üéØ Away Alta Confian√ßa", high_away)
        st.metric("üè† Home Oportunidades Fortes", strong_home)
        st.metric("üìà Total de Jogos", len(display_df))
        
        # Stake total recomendado
        total_stake = display_df['Stake Away'].sum() + display_df['Stake Home'].sum()
        st.metric("üí∞ Stake Total Recomendado", f"R$ {total_stake:.0f}")
        
        # Top oportunidades
        st.markdown("#### üî• Melhores Oportunidades Away")
        top_away = display_df[display_df['Confian√ßa Away'] == 'ALTA'][['Match', 'Prob Away HC']].head(3)
        for _, match in top_away.iterrows():
            st.write(f"**{match['Match']}** - {match['Prob Away HC']:.1%}")
        
        st.markdown("#### üè† Melhores Oportunidades Home")
        top_home = display_df[display_df['Oportunidade Home'] == 'FORTE'][['Match', 'Prob Home HC']].head(3)
        for _, match in top_home.iterrows():
            st.write(f"**{match['Match']}** - {match['Prob Home HC']:.1%}")

    ##################### BLOCO 10 ‚Äì DOWNLOAD DOS RESULTADOS #####################

    # Op√ß√£o para download
    csv = display_df.to_csv(index=False)
    st.download_button(
        label="üì• Download das Previs√µes",
        data=csv,
        file_name=f"previsoes_away_home_{selected_date_str}.csv",
        mime="text/csv"
    )

    st.success("‚úÖ Sistema Away Premium executado com sucesso!")

else:
    ##################### BLOCO 11 ‚Äì C√ìDIGO ORIGINAL (AMBOS OS LADOS) #####################
    
    # [SEU C√ìDIGO ORIGINAL AQUI - Blocos 4 a 8 do seu c√≥digo inicial]
    # ... (manter todo o c√≥digo original para a estrat√©gia "Original Both Sides")

    st.info("üîÅ Executando estrat√©gia original (ambos os lados)...")
    
    # [TODO: Inserir aqui os blocos 4 a 8 do seu c√≥digo original]
    # Por quest√µes de espa√ßo, mantive apenas a estrutura
    # Voc√™ pode copiar e colar seus blocos originais aqui

    st.warning("‚ö†Ô∏è Executando modo original - insira seus blocos 4-8 aqui")

# BLOCO EXTRA ‚Äì AN√ÅLISE DAS CORRELA√á√ïES (mantido para refer√™ncia)
st.markdown("### üîç An√°lise: Aggression vs Asian Handicap")

if not history.empty and all(col in history.columns for col in ['Target_AH_Home', 'Target_AH_Away']):
    
    # Features dispon√≠veis para an√°lise
    analysis_features = [f for f in away_premium_features if f in history.columns]
    
    if analysis_features:
        # Correla√ß√£o com target Home
        corr_home = history[analysis_features + ['Target_AH_Home']].corr()['Target_AH_Home'].drop('Target_AH_Home')
        
        # Correla√ß√£o com target Away  
        corr_away = history[analysis_features + ['Target_AH_Away']].corr()['Target_AH_Away'].drop('Target_AH_Away')
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Correla√ß√£o com Handicap Home Win:**")
            corr_df_home = pd.DataFrame({
                'Feature': corr_home.index,
                'Correlation': corr_home.values
            }).sort_values('Correlation', key=abs, ascending=False)
            st.dataframe(corr_df_home.style.format({'Correlation': '{:.3f}'}), use_container_width=True)
        
        with col2:
            st.write("**Correla√ß√£o com Handicap Away Win:**")
            corr_df_away = pd.DataFrame({
                'Feature': corr_away.index, 
                'Correlation': corr_away.values
            }).sort_values('Correlation', key=abs, ascending=False)
            st.dataframe(corr_df_away.style.format({'Correlation': '{:.3f}'}), use_container_width=True)
